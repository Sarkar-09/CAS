{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install statsmodels\n",
    "# !pip3 install gmpy2\n",
    "# !pip3 install cvxpy\n",
    "# !pip install Mosek\n",
    "# !pip install ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "import numpy as np\n",
    "import gmpy2\n",
    "from tqdm import tqdm\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data as GraphData\n",
    "import torch_geometric.datasets as pyg_datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device = {device}\")\n",
    "\n",
    "from tgnnu.data_utils.splits import SplitManager\n",
    "from tgnnu.networks.node_classif_models import GCN\n",
    "from tgnnu.networks.node_classif_lightner import NodeLevelGNN\n",
    "\n",
    "import gnn_cp.cp.transformations as cp_t\n",
    "import gnn_cp.cp.graph_transformations as cp_gt\n",
    "from gnn_cp.cp.graph_cp import GraphCP\n",
    "\n",
    "from graph_split import GraphSplit\n",
    "\n",
    "# import regions_binary\n",
    "import cvxpy as convex\n",
    "\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from utils import ModelManager\n",
    "from utils import standard_l2_norm\n",
    "\n",
    "# assignments\n",
    "datasets_folder = \"path_to_datasets\"\n",
    "models_direction = \"path_to_model\"\n",
    "from certify_utils import *\n",
    "import pickle\n",
    "\n",
    "def save_pkl(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_pkl(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_key = \"cora_ml\"\n",
    "model_key = \"GCN\"\n",
    "\n",
    "# cora_ml\n",
    "p_add = 0.01\n",
    "p_del = 0.6\n",
    "p_add_edge = 0.0\n",
    "p_del_edge = 0.0\n",
    " \n",
    "# laod graph datasets\n",
    "if dataset_key in [\"cora_ml\", \"pubmed\", \"citeseer\"]:\n",
    "    dataset = pyg_datasets.CitationFull(root=datasets_folder, name=dataset_key).data.to(device)\n",
    "if dataset_key in [\"Coauth-CS\", \"Coauth-Physics\"]:\n",
    "    dataset = pyg_datasets.Coauthor(root=datasets_folder, name=dataset_key.replace(\"Coauth-\", \"\")).data.to(device)\n",
    "if dataset_key in [\"Amz-Computers\", \"Amz-Photo\"]:\n",
    "    dataset = pyg_datasets.Amazon(root=datasets_folder, name=dataset_key.replace(\"Amz-\", \"\")).data.to(device)\n",
    "\n",
    "# Baseline Parameters\n",
    "if model_key == \"GCN\":\n",
    "    model_params = {\n",
    "        \"n_features\": dataset.x.shape[1], \n",
    "        \"n_hidden\": 64, \n",
    "        \"n_classes\": dataset.y.max().item() + 1, \n",
    "        \"p_dropout\": 0.6\n",
    "    }\n",
    "    optimizer_params = {\"weight_decay\": 1e-2}\n",
    "\n",
    "model_r = GCN(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NodeLevelGNN(model_r)\n",
    "model.set_optimizer(optimizer_keyargs=optimizer_params)\n",
    "\n",
    "\n",
    "training_budget = 20\n",
    "calibration_budget = training_budget * (dataset.y.max().item() + 1)\n",
    "\n",
    "\n",
    "main_split = GraphSplit.from_dataset(dataset)\n",
    "training_mask = main_split.sample_nodes(training_budget, stratified=True)\n",
    "validation_mask = main_split.sample_nodes(training_budget, stratified=True)\n",
    "test_mask = ~(training_mask | validation_mask)\n",
    "\n",
    "dataset.train_mask = training_mask\n",
    "dataset.val_mask = validation_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_to_str = lambda p: str(p).replace(\".\", \"_\")\n",
    "\n",
    "s_model_loaded = False\n",
    "try:\n",
    "    model.model.load_state_dict(\n",
    "        torch.load(os.path.join(\n",
    "            models_direction, \n",
    "            f\"{model_key}-{dataset_key}-{param_to_str(p_add)}-{param_to_str(p_del)}-{param_to_str(p_add_edge)}-{param_to_str(p_del_edge)}.pth\")))\n",
    "    s_model_loaded = True\n",
    "    masks_state_dict = torch.load(os.path.join(\n",
    "        models_direction, \n",
    "        f\"{model_key}-{dataset_key}-{param_to_str(p_add)}-{param_to_str(p_del)}-{param_to_str(p_add_edge)}-{param_to_str(p_del_edge)}-masks.pth\"))\n",
    "    dataset.train_mask = masks_state_dict[\"train_mask\"]\n",
    "    training_mask = dataset.train_mask\n",
    "    dataset.val_mask = masks_state_dict[\"val_mask\"]\n",
    "    validation_mask = dataset.val_mask\n",
    "    test_mask = ~(training_mask | validation_mask)\n",
    "    print(\"Model loaded\")\n",
    "except:\n",
    "    print(\"Unable to load model\")\n",
    "    tloss, vloss = model.fit(dataset, epochs=1000, patience=20, \n",
    "        smoothing_lambda=lambda d: standard_sparse_smoothing_concat(d, n_samples=4, p_add=p_add, p_del=p_del, p_add_edge=p_add_edge, p_del_edge=p_del_edge), smoothing_attrs=True)\n",
    "    pathlib.Path(models_direction).mkdir(exist_ok=True)\n",
    "    torch.save(model.model.state_dict(), os.path.join(\n",
    "        models_direction, \n",
    "        f\"{model_key}-{dataset_key}-{param_to_str(p_add)}-{param_to_str(p_del)}-{param_to_str(p_add_edge)}-{param_to_str(p_del_edge)}.pth\"))\n",
    "    # saving train and val masks\n",
    "    masks_state_dict = {\n",
    "        \"train_mask\": dataset.train_mask,\n",
    "        \"val_mask\": dataset.val_mask\n",
    "    }\n",
    "    torch.save(masks_state_dict, os.path.join(\n",
    "        models_direction, \n",
    "        f\"{model_key}-{dataset_key}-{param_to_str(p_add)}-{param_to_str(p_del)}-{param_to_str(p_add_edge)}-{param_to_str(p_del_edge)}-masks.pth\"))\n",
    "\n",
    "accuracy = model.evaluate(dataset, dataset.y, test_mask)\n",
    "print(f\"Accuracy of one-time prediction = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_logits = model.smooth_predict(dataset, n_samples=10000, \n",
    "                     smoothing_function=lambda input: standard_sparse_smoothing(\n",
    "                         input=input, p_add=p_add, p_del=p_del, p_add_edge=p_add_edge, p_del_edge=p_del_edge), \n",
    "                     mask=None)\n",
    "\n",
    "torch.save(smooth_logits, \n",
    "           os.path.join(models_direction, \n",
    "                        f\"{model_key}-{dataset_key}-{param_to_str(p_add)}-{param_to_str(p_del)}-{param_to_str(p_add_edge)}-{param_to_str(p_del_edge)}-smooth_logits.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Conformal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_logits= torch.load( \n",
    "           os.path.join(models_direction, \n",
    "                        f\"{model_key}-{dataset_key}-{param_to_str(p_add)}-{param_to_str(p_del)}-{param_to_str(p_add_edge)}-{param_to_str(p_del_edge)}-smooth_logits.pth\"),\n",
    "                        map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                              | 0/2995 [00:00<?, ?it/s]TBB Warning: The number of workers is currently limited to 15. The request for 255 workers is ignored. Further requests for more workers will be silently ignored until the limit changes.\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2995/2995 [20:59<00:00,  2.38it/s]\n"
     ]
    }
   ],
   "source": [
    "r_add = 0\n",
    "r_del = 3\n",
    "r_add_edge = 0\n",
    "r_del_edge = 0\n",
    "coverage_guarantees = np.arange(0.7, 1, 0.05)\n",
    "coverage_guarantees\n",
    "\n",
    "scoreing= 'APS'\n",
    "for r_del in range(4):\n",
    "    print('fine')\n",
    "\n",
    "    smooth_logits= torch.load( \n",
    "               os.path.join(models_direction, \n",
    "                            f\"{model_key}-{dataset_key}-{param_to_str(p_add)}-{param_to_str(p_del)}-{param_to_str(p_add_edge)}-{param_to_str(p_del_edge)}-smooth_logits.pth\"),\n",
    "                            map_location=torch.device('cpu'))\n",
    "\n",
    "    y_true_mask = F.one_hot(dataset.y).bool()\n",
    "   \n",
    "    #TPS\n",
    "    if scoreing== 'TPS':\n",
    "        cp = GraphCP(transformation_sequence=[cp_t.TPSTransformation(softmax=True)], coverage_guarantee=0.9)\n",
    "        sc_scores = torch.stack([cp.get_scores_from_logits(smooth_logits[:, i, :]) for i in range(smooth_logits.shape[1])]).permute(1, 2, 0)\n",
    "    else:\n",
    "    #APS\n",
    "        cp = GraphCP(transformation_sequence=[cp_t.APSTransformation(softmax=True)], coverage_guarantee=0.9)\n",
    "        sc_scores = torch.stack([cp.get_scores_from_logits(smooth_logits[:, i, :]) for i in range(smooth_logits.shape[1])]).permute(1, 2, 0)  + 1\n",
    "\n",
    "    esc_scores = sc_scores.mean(axis=2)\n",
    "    esc_scores.shape\n",
    "   \n",
    "    dkw_score_upperbound = torch.stack([torch.tensor([\n",
    "    dkw_offset(sc_scores, node_i, class_i, pf_plus_att=p_add, pf_minus_att=p_del, ra=r_add, rd=r_del, num_s=2000, alpha=0.1) \n",
    "    for class_i in range(dataset.y.max().item() + 1)\n",
    "    ]) for node_i in tqdm(range(dataset.x.shape[0]))]).to(device)\n",
    "    \n",
    "    np_score_upperbound = torch.stack([torch.tensor([\n",
    "    np_offset(sc_scores, node_i, class_i, pf_plus_att=p_add, pf_minus_att=p_del, ra=r_add, rd=r_del, alpha=0.1) \n",
    "    for class_i in range(dataset.y.max().item() + 1)\n",
    "    ]) for node_i in range(dataset.x.shape[0])]).to(device)\n",
    "\n",
    "    args = EasyDict(model_key= model_key, dataset_key=dataset_key, p_add=p_add, p_del=p_del, p_add_edge= p_add_edge,\n",
    "                p_del_edge=p_del_edge, r_add=r_add, r_del=r_del, r_add_edge=r_add_edge, r_del_edge=r_del_edge)\n",
    "    \n",
    "    save_pkl((args, np_score_upperbound.cpu(), dkw_score_upperbound.cpu()),\n",
    "         path=os.path.join(models_direction, \n",
    "                        f\"{model_key}-{dataset_key}-{param_to_str(p_add)}-{param_to_str(p_del)}-{param_to_str(p_add_edge)}-{param_to_str(p_del_edge)}-rs-{r_add}-{r_del}-{r_add_edge}-{r_del_edge}-upper_bouds_new.pth\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for coverage_guarantee in coverage_guarantees:\n",
    "\n",
    "        cp = GraphCP(transformation_sequence=[cp_t.APSTransformation(softmax=True)], coverage_guarantee=coverage_guarantee)\n",
    "        for iter_i in range(100):\n",
    "            split = GraphSplit(dataset.x.shape[0], n_edges=dataset.edge_index.shape[1], edge_index=dataset.edge_index, device=device)\n",
    "\n",
    "            split._vertices_budget[training_mask | validation_mask] = False\n",
    "            cal_mask = split.sample_nodes(calibration_budget, stratified=False)\n",
    "\n",
    "            # COMMENT THIS:\n",
    "            limiter_mask = torch.ones_like(cal_mask)\n",
    "            # limiter_mask[:200] = True\n",
    "\n",
    "            threshold = cp.calibrate_from_scores(esc_scores[cal_mask], y_true_mask[cal_mask])\n",
    "\n",
    "            np_pred_set = (np_score_upperbound > threshold)\n",
    "            dkw_pred_set = (dkw_score_upperbound > threshold)\n",
    "\n",
    "            eval_mask = (~cal_mask) & limiter_mask\n",
    "\n",
    "            result.append({\n",
    "                \"p_add_attr\": p_add,\n",
    "                \"p_del_attr\": p_del,\n",
    "                \"p_add_adj\": p_add_edge,\n",
    "                \"p_del_adj\": p_del_edge,\n",
    "                \"r_add_attr\": r_add,\n",
    "                \"r_del_attr\": r_del,\n",
    "                \"r_add_adj\": r_add_edge,\n",
    "                \"r_del_adj\": r_del_edge,\n",
    "                \"iter\": iter_i,\n",
    "                \"method\": \"NP\",\n",
    "                \"$1-\\\\alpha$\": coverage_guarantee,\n",
    "                \"coverage\": cp.coverage(np_pred_set[eval_mask], y_true_mask[eval_mask]),\n",
    "                \"set_size\": cp.average_set_size(np_pred_set[eval_mask]),\n",
    "                \"singleton_hits\": singleton_hit(np_pred_set[eval_mask], y_true_mask[eval_mask]),\n",
    "            })\n",
    "\n",
    "            result.append({\n",
    "                \"p_add_attr\": p_add,\n",
    "                \"p_del_attr\": p_del,\n",
    "                \"p_add_adj\": p_add_edge,\n",
    "                \"p_del_adj\": p_del_edge,\n",
    "                \"r_add_attr\": r_add,\n",
    "                \"r_del_attr\": r_del,\n",
    "                \"r_add_adj\": r_add_edge,\n",
    "                \"r_del_adj\": r_del_edge,\n",
    "                \"iter\": iter_i,\n",
    "                \"method\": \"DKW\",\n",
    "                \"$1-\\\\alpha$\": coverage_guarantee,\n",
    "                \"coverage\": cp.coverage(dkw_pred_set[eval_mask], y_true_mask[eval_mask]),\n",
    "                \"set_size\": cp.average_set_size(dkw_pred_set[eval_mask]),\n",
    "                \"singleton_hits\": singleton_hit(dkw_pred_set[eval_mask], y_true_mask[eval_mask]),\n",
    "            })\n",
    "    result = pd.DataFrame(result)\n",
    "    result.to_csv(os.path.join(models_direction, f\"{scoreing}-results-cora_{r_del}.csv\"), index=False)\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"NP Cov: {cp.coverage(np_pred_set[eval_mask[:200]], y_true_mask[eval_mask])}, DKW Cov: {cp.coverage(dkw_pred_set[eval_mask[:200]], y_true_mask[eval_mask])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
